# -*- coding: utf-8 -*-
"""Wellpack.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VF0e3IbXAY-PDT3JEC8Qjyqg52hTQAle
"""

import requests
import zipfile
import io
import json
from tabulate import tabulate

api_key = "2e5a7999-a406-4eca-8a54-22a115583e54"
flux_id = "e22e20b5ab01520a013f8411fa327be0"

a = api_key
f = flux_id

url = f"https://diffuseur.datatourisme.fr/webservice/{f}/{a}"

r = requests.get(url)

# --- | The response is a ZIP file | ---
zip_bytes = io.BytesIO(r.content)

with zipfile.ZipFile(zip_bytes) as z:
    print("Files inside ZIP ğŸ“:")

    # --- | Read first JSON inside the ZIP | ---
    filename = z.namelist()[0]
    with z.open(filename) as f:
        json_text = f.read().decode("utf-8")

# --- | Fixed Data | ---
data = json.loads(json_text)

print("JSON Parsed successfully! âœ…ğŸ‰")
print(type(data))

# View complete JSON for nth event
n = 21  # Change this to see different events

with zipfile.ZipFile(zip_bytes) as z:
    filename = z.namelist()[n]
    with z.open(filename) as f:
        json_text = f.read().decode("utf-8")
        event_data = json.loads(json_text)

print(f"ğŸ“„ Complete JSON for Event #{n+1}: {filename}")
print("="*80)
print()
print(json.dumps(event_data, indent=2, ensure_ascii=False))

# --- | Inspecting the Structure of One Event to See What's Actually There | ---

with zipfile.ZipFile(zip_bytes) as z:
    filename = z.namelist()[0]
    with z.open(filename) as f:
        json_text = f.read().decode("utf-8")
        sample_event = json.loads(json_text)

# --- | Print the keys to see what's available | ---
print("Available keys in the event:")
print(json.dumps(list(sample_event.keys()), indent=2))

# use = int(input("Enter a Number Between 1 and 2: "))
use = 2
if use == 1:
    # Let's look at a few potentially relevant fields
    if "rdfs:label" in sample_event:
        print("Label:", sample_event["rdfs:label"])

    if "schema:location" in sample_event:
        print("\nLocation:", sample_event["schema:location"])

    if "hasBeenPublishedBy" in sample_event:
        print("\nPublished by:", sample_event["hasBeenPublishedBy"])

    if "isLocatedAt" in sample_event:
        print("\nIs located at:", sample_event["isLocatedAt"])

    # Print dates
    for key in sample_event.keys():
        if "date" in key.lower() or "Date" in key:
            print(f"\n{key}:", sample_event[key])


elif use == 2:
    # Let's look at a few potentially relevant fields
    if "rdfs:label" in sample_event:
        print("ğŸ“ **Label:**", sample_event["rdfs:label"].get('fr', ['N/A'])[0])

    if "schema:location" in sample_event:
        print("\nğŸ“ **Location:**", sample_event["schema:location"])

    if "hasBeenPublishedBy" in sample_event:
        print("\nğŸ¢ **Published by:**")
        publisher = sample_event["hasBeenPublishedBy"][0]
        print(f"  - Organization: {publisher.get('schema:legalName', 'N/A')}")
        print(f"  - Website: {publisher.get('foaf:homepage', ['N/A'])[0]}")

    if "isLocatedAt" in sample_event:
        print("\nğŸ“ **Is located at:**")
        location = sample_event["isLocatedAt"][0]
        address = location.get('schema:address', [{}])[0]
        print(f"  - Address: {', '.join(address.get('schema:streetAddress', ['N/A']))}")
        print(f"  - City: {address.get('schema:addressLocality', 'N/A')}")
        print(f"  - Postal Code: {address.get('schema:postalCode', 'N/A')}")
        print(f"  - Region: {address.get('isPartOfRegion', {}).get('rdfs:label', {}).get('en', 'N/A')}")
        print(f"  - Country: {address.get('isPartOfCountry', {}).get('rdfs:label', {}).get('en', 'N/A')}")
    # Print dates
    print("\nğŸ“… **Dates:**")
    for key in sample_event.keys():
        if "date" in key.lower() or "Date" in key:
            print(f"  - {key}: {sample_event[key]}")

else :
  print("Invalid Input")

# Specify which event you want to see (nth event, starting from 0)
n = 0 # Change this to see different events (0 for first, 1 for second, etc.)

# Get the nth event from the ZIP
with zipfile.ZipFile(zip_bytes) as z:
    filename = z.namelist()[n]
    with z.open(filename) as f:
        json_text = f.read().decode("utf-8")
        event_data = json.loads(json_text)

print(f"ğŸ“ EVENT #{n+1}: {filename}")
print("="*80)
print()

# Function to flatten nested data for display
def format_value(value, indent=0):
    """Format values for better readability"""
    if isinstance(value, dict):
        if 'fr' in value:
            return value['fr'][0] if isinstance(value['fr'], list) else value['fr']
        return json.dumps(value, indent=2, ensure_ascii=False)
    elif isinstance(value, list):
        if len(value) > 0 and isinstance(value[0], dict):
            return json.dumps(value, indent=2, ensure_ascii=False)
        return ', '.join(str(v) for v in value)
    return str(value)

# Prepare main event details
main_details = []
for key, value in event_data.items():
    if key not in ['isLocatedAt', 'hasContact', 'hasDescription', 'takesPlaceAt', 'hasTranslatedProperty']:
        formatted_value = format_value(value)
        # Truncate very long values
        if len(formatted_value) > 100:
            formatted_value = formatted_value[:100] + "..."
        main_details.append([key, formatted_value])

print("ğŸ“‹ MAIN DETAILS:")
print(tabulate(main_details, headers=['Field', 'Value'], tablefmt='grid', maxcolwidths=[30, 80]))
print()

# Location details
if event_data.get("isLocatedAt"):
    location_data = event_data["isLocatedAt"][0]
    location_details = []

    if location_data.get("schema:address"):
        address = location_data["schema:address"][0]
        location_details.append(["City", address.get("schema:addressLocality", "N/A")])
        location_details.append(["Postal Code", address.get("schema:postalCode", "N/A")])
        location_details.append(["Street", ', '.join(address.get("schema:streetAddress", ["N/A"]))])

    if location_data.get("schema:geo"):
        geo = location_data["schema:geo"]
        location_details.append(["Latitude", geo.get("schema:latitude", "N/A")])
        location_details.append(["Longitude", geo.get("schema:longitude", "N/A")])

    print("ğŸ“ LOCATION:")
    print(tabulate(location_details, headers=['Field', 'Value'], tablefmt='grid'))
    print()

# Contact details
if event_data.get("hasContact"):
    contact_data = event_data["hasContact"][0]
    contact_details = []

    for key, value in contact_data.items():
        if not key.startswith('@'):
            formatted_value = format_value(value)
            if len(formatted_value) > 100:
                formatted_value = formatted_value[:100] + "..."
            contact_details.append([key, formatted_value])

    if contact_details:
        print("ğŸ“ CONTACT:")
        print(tabulate(contact_details, headers=['Field', 'Value'], tablefmt='grid', maxcolwidths=[30, 80]))
        print()

# Description
if event_data.get("hasDescription"):
    desc_data = event_data["hasDescription"][0]
    print("ğŸ“ DESCRIPTION:")
    if desc_data.get("dc:description"):
        desc = desc_data["dc:description"].get("fr", ["N/A"])[0]
        print(f"{desc[:500]}..." if len(desc) > 500 else desc)
    print()

# Schedule/Opening hours
if event_data.get("isLocatedAt") and event_data["isLocatedAt"][0].get("schema:openingHoursSpecification"):
    schedule = event_data["isLocatedAt"][0]["schema:openingHoursSpecification"]
    schedule_details = []

    for slot in schedule[:5]:  # Show first 5 time slots
        schedule_details.append([
            slot.get("schema:validFrom", "N/A"),
            slot.get("schema:opens", "N/A"),
            slot.get("schema:closes", "N/A")
        ])

    if schedule_details:
        print("ğŸ• SCHEDULE:")
        print(tabulate(schedule_details, headers=['Date', 'Opens', 'Closes'], tablefmt='grid'))
        print()

# Extract first n events with selected fields
events = []
n = 10
# Get first 10 JSON files from the ZIP
for filename in z.namelist()[:n]:
    with zipfile.ZipFile(zip_bytes) as z:
        with z.open(filename) as f:
            json_text = f.read().decode("utf-8")
            event_data = json.loads(json_text)

            # Extract label
            label = event_data.get("rdfs:label", {}).get("fr", ["N/A"])[0]

            # Extract location from isLocatedAt
            location = "N/A"
            if event_data.get("isLocatedAt"):
                address = event_data["isLocatedAt"][0].get("schema:address", [{}])[0]
                location = address.get("schema:addressLocality", "N/A")
                location += ", " + address.get("schema:postalCode", "N/A")

            # Extract dates
            start_date = event_data.get("schema:startDate", ["N/A"])[0]
            end_date = event_data.get("schema:endDate", ["N/A"])[0]

            event = {
                "label": label,
                "location": location,
                "start_date": start_date,
                "end_date": end_date
            }
            events.append(event)

# Display the events
for i, event in enumerate(events, 1):
    print(f"{i}. {event['label']}")
    print(f"   Location: {event['location']}")
    print(f"   Start: {event['start_date']}")
    print(f"   End: {event['end_date']}")
    print()

# view = int(input("Enter a Number Between 1 and 3: "))
view = 2

if view == 1:
  import pandas as pd

  # Create DataFrame from events list
  df = pd.DataFrame(events)

  # Display with better column names
  df.columns = ['Event Name', 'Location', 'Start Date', 'End Date']
  df.index = range(1, len(df) + 1)

  display(df)


elif view == 2:
  # Create table
  table_data = []
  for i, event in enumerate(events, 1):
      table_data.append([
          i,
          event['label'],
          event['location'],
          event['start_date'],
          event['end_date']
      ])

  print(tabulate(table_data,
                headers=['#', 'Event Name', 'Location', 'Start Date', 'End Date'],
                tablefmt='grid'))


elif view == 3:
  # Print header
  print(f"{'#':<4} {'Event Name':<50} {'Location':<20} {'Start Date':<12} {'End Date':<12}")
  print("-" * 100)

  # Print rows
  for i, event in enumerate(events, 1):
      label = event['label'][:47] + '...' if len(event['label']) > 50 else event['label']
      location = event['location'][:17] + '...' if len(event['location']) > 20 else event['location']

      print(f"{i:<4} {label:<50} {location:<20} {event['start_date']:<12} {event['end_date']:<12}")


else :
  print("Invalid Input")

# Extract first n events with MORE selected fields including region and department
events = []
n = 4
# Get first 10 JSON files from the ZIP
for filename in z.namelist()[:n]:
    with zipfile.ZipFile(zip_bytes) as z:
        with z.open(filename) as f:
            json_text = f.read().decode("utf-8")
            event_data = json.loads(json_text)

            # Extract label
            label = event_data.get("rdfs:label", {}).get("fr", ["N/A"])[0]

            # Extract location details from isLocatedAt
            location = "N/A"
            postal_code = "N/A"
            department = "N/A"
            region = "N/A"
            latitude = "N/A"
            longitude = "N/A"

            if event_data.get("isLocatedAt"):
                location_data = event_data["isLocatedAt"][0]

                # Get address
                if location_data.get("schema:address"):
                    address = location_data["schema:address"][0]
                    location = address.get("schema:addressLocality", "N/A")
                    postal_code = address.get("schema:postalCode", "N/A")

                    # Get department and region from nested structure
                    if address.get("hasAddressCity"):
                        city_data = address["hasAddressCity"]

                        if city_data.get("isPartOfDepartment"):
                            dept_data = city_data["isPartOfDepartment"]
                            department = dept_data.get("rdfs:label", {}).get("fr", "N/A")

                            if dept_data.get("isPartOfRegion"):
                                region_data = dept_data["isPartOfRegion"]
                                region = region_data.get("rdfs:label", {}).get("fr", "N/A")

                # Get coordinates
                if location_data.get("schema:geo"):
                    geo = location_data["schema:geo"]
                    latitude = geo.get("schema:latitude", "N/A")
                    longitude = geo.get("schema:longitude", "N/A")

            # Extract dates
            start_date = event_data.get("schema:startDate", ["N/A"])[0]
            end_date = event_data.get("schema:endDate", ["N/A"])[0]

            # Extract description
            description = "N/A"
            if event_data.get("hasDescription"):
                desc_data = event_data["hasDescription"][0]
                if desc_data.get("dc:description"):
                    description = desc_data["dc:description"].get("fr", ["N/A"])[0]
                    # Truncate long descriptions
                    if len(description) > 150:
                        description = description[:150] + "..."

            event = {
                "label": label,
                "location": location,
                "postal_code": postal_code,
                "department": department,
                "region": region,
                "start_date": start_date,
                "end_date": end_date,
                "latitude": latitude,
                "longitude": longitude,
                "description": description
            }
            events.append(event)

# Display with pandas for better formatting
import pandas as pd

df = pd.DataFrame(events)
df.index = range(1, len(df) + 1)

# Display with better column names
df.columns = ['Event Name', 'City', 'Postal Code', 'Department', 'Region',
              'Start Date', 'End Date', 'Latitude', 'Longitude', 'Description']

display(df)

"""_________________

"""

import requests
import zipfile
import io
import json
from collections import Counter

# Your API credentials
api_key = "2e5a7999-a406-4eca-8a54-22a115583e54"
flux_id = "e22e20b5ab01520a013f8411fa327be0"

url = f"https://diffuseur.datatourisme.fr/webservice/{flux_id}/{api_key}"
r = requests.get(url)
zip_bytes = io.BytesIO(r.content)

# Explore event types and categories
event_types = []
categories = []
locations = []

print("ğŸ” Analyzing first 50 events to understand data structure...\n")

with zipfile.ZipFile(zip_bytes) as z:
    for i, filename in enumerate(z.namelist()[:50]):
        with z.open(filename) as f:
            json_text = f.read().decode("utf-8")
            event_data = json.loads(json_text)

            # Collect event types
            if "@type" in event_data:
                event_types.extend(event_data["@type"])

            # Collect location info
            if event_data.get("isLocatedAt"):
                location_data = event_data["isLocatedAt"][0]
                if location_data.get("schema:address"):
                    address = location_data["schema:address"][0]
                    city = address.get("schema:addressLocality", "")
                    if city:
                        locations.append(city)

# Count occurrences
type_counts = Counter(event_types)
location_counts = Counter(locations)

print("ğŸ“Š EVENT TYPES FOUND:")
print("=" * 60)
for event_type, count in type_counts.most_common(20):
    print(f"  {event_type}: {count}")

print("\nğŸ“ TOP CITIES FOUND:")
print("=" * 60)
for city, count in location_counts.most_common(15):
    print(f"  {city}: {count}")

print("\nğŸ’¡ Sample event structure keys:")
print("=" * 60)
with zipfile.ZipFile(zip_bytes) as z:
    with z.open(z.namelist()[0]) as f:
        sample = json.loads(f.read().decode("utf-8"))
        for key in sample.keys():
            print(f"  - {key}")

import requests
import zipfile
import io
import json
from datetime import datetime
from tabulate import tabulate

# API credentials
api_key = "2e5a7999-a406-4eca-8a54-22a115583e54"
flux_id = "e22e20b5ab01520a013f8411fa327be0"

# Industry mapping - maps event types to industries
INDUSTRY_MAPPING = {
    'Market': 'Retail/Food',
    'SaleEvent': 'Retail',
    'schema:SaleEvent': 'Retail',
    'Concert': 'Entertainment',
    'schema:MusicEvent': 'Entertainment',
    'TheaterEvent': 'Entertainment',
    'schema:TheaterEvent': 'Entertainment',
    'Exhibition': 'Arts/Culture',
    'schema:ExhibitionEvent': 'Arts/Culture',
    'CulturalEvent': 'Arts/Culture',
    'SportsEvent': 'Sports/Fitness',
    'schema:SportsEvent': 'Sports/Fitness',
    'SportsCompetition': 'Sports/Fitness',
    'Visit': 'Tourism',
    'ShowEvent': 'Entertainment'
}

# Campaign sector impact rules
IMPACT_RULES = {
    'food': {
        'positive': ['Retail/Food', 'Entertainment', 'Tourism'],
        'negative': ['Sports/Fitness'],
        'neutral': ['Arts/Culture', 'Retail']
    },
    'restaurant': {
        'positive': ['Entertainment', 'Tourism', 'Arts/Culture'],
        'negative': ['Retail/Food'],
        'neutral': ['Sports/Fitness', 'Retail']
    },
    'retail': {
        'positive': ['Tourism', 'Entertainment', 'Arts/Culture'],
        'negative': ['Retail', 'Retail/Food'],
        'neutral': ['Sports/Fitness']
    },
    'fitness': {
        'positive': ['Sports/Fitness'],
        'negative': ['Retail/Food'],
        'neutral': ['Entertainment', 'Tourism', 'Arts/Culture', 'Retail']
    },
    'entertainment': {
        'positive': ['Arts/Culture', 'Tourism'],
        'negative': ['Entertainment'],
        'neutral': ['Retail', 'Sports/Fitness', 'Retail/Food']
    },
    'healthcare': {
        'positive': ['Sports/Fitness'],
        'negative': ['Entertainment', 'Retail/Food'],
        'neutral': ['Tourism', 'Arts/Culture', 'Retail']
    },
    'tourism': {
        'positive': ['Arts/Culture', 'Entertainment', 'Tourism'],
        'negative': [],
        'neutral': ['Retail', 'Sports/Fitness', 'Retail/Food']
    },
    'education': {
        'positive': ['Arts/Culture'],
        'negative': ['Entertainment'],
        'neutral': ['Tourism', 'Retail', 'Sports/Fitness', 'Retail/Food']
    }
}

def safe_extract_list_value(data, default=""):
    """Safely extract value from list or dict"""
    if isinstance(data, list) and len(data) > 0:
        return data[0]
    elif isinstance(data, str):
        return data
    return default

def safe_extract_label(label_data):
    """Safely extract French label from various formats"""
    try:
        if isinstance(label_data, dict):
            fr_data = label_data.get("fr", "N/A")
            if isinstance(fr_data, list) and len(fr_data) > 0:
                return fr_data[0]
            elif isinstance(fr_data, str):
                return fr_data
        return "N/A"
    except:
        return "N/A"

def get_event_industry(event_types):
    """Determine the industry of an event based on its types"""
    if not isinstance(event_types, list):
        return 'General'

    for event_type in event_types:
        if event_type in INDUSTRY_MAPPING:
            return INDUSTRY_MAPPING[event_type]
    return 'General'

def assess_impact(event_industry, campaign_sector):
    """Assess the impact of an event on a campaign"""
    sector_rules = IMPACT_RULES.get(campaign_sector.lower(), {})

    if event_industry in sector_rules.get('positive', []):
        return 'Positive', 'Attracts relevant audience'
    elif event_industry in sector_rules.get('negative', []):
        return 'Negative', 'Competing or conflicting'
    else:
        return 'Neutral', 'Minimal direct impact'

def analyze_campaign(target_date, target_city, campaign_sector):
    """Main function to analyze campaign impact"""

    print(f"\n{'='*80}")
    print(f"ğŸ“‹ CAMPAIGN IMPACT ANALYSIS")
    print(f"{'='*80}")
    print(f"ğŸ“… Date: {target_date}")
    print(f"ğŸ“ City: {target_city}")
    print(f"ğŸ¯ Sector: {campaign_sector.upper()}")
    print(f"{'='*80}\n")

    # Fetch data
    print("â³ Fetching event data...")
    try:
        url = f"https://diffuseur.datatourisme.fr/webservice/{flux_id}/{api_key}"
        r = requests.get(url, timeout=30)
        zip_bytes = io.BytesIO(r.content)
    except Exception as e:
        print(f"âŒ Error fetching data: {e}")
        return

    # Find matching events
    matching_events = []
    processed_count = 0

    try:
        with zipfile.ZipFile(zip_bytes) as z:
            total_files = len(z.namelist())
            print(f"ğŸ“¦ Processing {total_files} events...")

            for filename in z.namelist():
                try:
                    with z.open(filename) as f:
                        json_text = f.read().decode("utf-8")
                        event_data = json.loads(json_text)
                        processed_count += 1

                        # Progress indicator every 100 events
                        if processed_count % 100 == 0:
                            print(f"   Processed {processed_count}/{total_files} events...")

                        # Extract and validate dates
                        start_date_raw = event_data.get("schema:startDate")
                        end_date_raw = event_data.get("schema:endDate")

                        start_date = safe_extract_list_value(start_date_raw)
                        end_date = safe_extract_list_value(end_date_raw)

                        # Skip if no valid dates
                        if not start_date or not end_date:
                            continue

                        # Check date match
                        if target_date >= start_date and target_date <= end_date:
                            # Check city match
                            if event_data.get("isLocatedAt"):
                                location_data = event_data["isLocatedAt"][0] if isinstance(event_data["isLocatedAt"], list) else event_data["isLocatedAt"]

                                if location_data.get("schema:address"):
                                    address_data = location_data["schema:address"]
                                    address = address_data[0] if isinstance(address_data, list) else address_data
                                    city = address.get("schema:addressLocality", "")

                                    if target_city.lower() in city.lower():
                                        # Extract event details
                                        event_name = safe_extract_label(event_data.get("rdfs:label", {}))
                                        event_types = event_data.get("@type", [])
                                        industry = get_event_industry(event_types)
                                        impact, reason = assess_impact(industry, campaign_sector)

                                        matching_events.append({
                                            'name': event_name,
                                            'date': start_date,
                                            'industry': industry,
                                            'impact': impact,
                                            'reason': reason,
                                            'location': city
                                        })
                except Exception as e:
                    # Skip problematic events silently
                    continue

    except Exception as e:
        print(f"âŒ Error processing data: {e}")
        return

    print(f"\nâœ… Found {len(matching_events)} events matching your criteria\n")

    if not matching_events:
        print("âŒ No events found for the specified date and city.")
        print("\nğŸ’¡ Tips:")
        print("   - Try a different date (e.g., 2025-03-10, 2025-05-30)")
        print("   - Check the city name spelling")
        print("   - Try these cities: Paris, Cormeilles-en-Parisis, Fontainebleau, Versailles")
        return

    # Prepare table data
    table_data = []
    positive_count = 0
    negative_count = 0
    neutral_count = 0

    for i, event in enumerate(matching_events, 1):
        # Count impacts
        if event['impact'] == 'Positive':
            positive_count += 1
            impact_icon = 'âœ…'
        elif event['impact'] == 'Negative':
            negative_count += 1
            impact_icon = 'âš ï¸'
        else:
            neutral_count += 1
            impact_icon = 'â–'

        table_data.append([
            i,
            event['name'][:40] + '...' if len(event['name']) > 40 else event['name'],
            event['industry'],
            f"{impact_icon} {event['impact']}",
            event['reason']
        ])

    # Display results
    print(tabulate(
        table_data,
        headers=['Sr.No.', 'Event Name', 'Industry', 'Impact', 'Reason'],
        tablefmt='grid',
        maxcolwidths=[6, 40, 15, 15, 25]
    ))

    # Summary
    print(f"\n{'='*80}")
    print("ğŸ“Š IMPACT SUMMARY")
    print(f"{'='*80}")
    print(f"âœ… Positive Impact Events: {positive_count} ({positive_count/len(matching_events)*100:.1f}%)")
    print(f"âš ï¸  Negative Impact Events: {negative_count} ({negative_count/len(matching_events)*100:.1f}%)")
    print(f"â– Neutral Impact Events: {neutral_count} ({neutral_count/len(matching_events)*100:.1f}%)")
    print(f"ğŸ“ˆ Total Events: {len(matching_events)}")

    # Recommendation
    print(f"\n{'='*80}")
    print("ğŸ’¡ RECOMMENDATION")
    print(f"{'='*80}")

    positive_ratio = positive_count / len(matching_events)
    negative_ratio = negative_count / len(matching_events)

    if positive_ratio > 0.5:
        print("ğŸŸ¢ EXCELLENT DAY for your campaign!")
        print("   Many events will attract your target audience.")
    elif positive_ratio > 0.3 and negative_ratio < 0.3:
        print("ğŸŸ¡ GOOD DAY for your campaign.")
        print("   Decent event alignment with manageable competition.")
    elif negative_ratio > 0.5:
        print("ğŸ”´ CHALLENGING DAY for your campaign.")
        print("   Consider alternative dates or adjust messaging.")
    else:
        print("ğŸŸ¡ MODERATE DAY for your campaign.")
        print("   Standard conditions, focus on differentiation.")

    print(f"{'='*80}\n")


# ========== INTERACTIVE CAMPAIGN PLANNER ==========

print("\n" + "="*80)
print("ğŸ¯ CAMPAIGN IMPACT ANALYZER")
print("="*80)
print("\nâœ… TESTED INPUT COMBINATIONS:")
print("   ğŸ“… Date: 2025-01-27  |  ğŸ“ City: Cormeilles-en-Parisis  |  ğŸ¯ Sector: entertainment")
print("   ğŸ“… Date: 2025-03-10  |  ğŸ“ City: Paris  |  ğŸ¯ Sector: food")
print("   ğŸ“… Date: 2025-05-30  |  ğŸ“ City: Fontainebleau  |  ğŸ¯ Sector: tourism")
print("   ğŸ“… Date: 2025-01-01  |  ğŸ“ City: Drancy  |  ğŸ¯ Sector: retail")
print("\nğŸ“‹ Available sectors:")
print("   food, restaurant, retail, fitness, entertainment, healthcare, tourism, education")
print("="*80 + "\n")

# Get user input
target_date = input("ğŸ“… Enter campaign date (YYYY-MM-DD): ").strip()
target_city = input("ğŸ“ Enter city name: ").strip()
campaign_sector = input("ğŸ¯ Enter your business sector: ").strip()

# Validate date format
try:
    datetime.strptime(target_date, '%Y-%m-%d')
except ValueError:
    print("\nâŒ Invalid date format. Please use YYYY-MM-DD (e.g., 2025-03-15)")
    exit()

# Validate sector
if campaign_sector.lower() not in IMPACT_RULES:
    print(f"\nâš ï¸  Warning: '{campaign_sector}' not in predefined sectors.")
    print("   Analysis will use general impact assessment.")

# Run analysis
analyze_campaign(target_date, target_city, campaign_sector)